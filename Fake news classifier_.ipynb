{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f51ce73-11fa-4575-9180-da483131f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 2.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "     |████████████████████████████████| 749 kB 60.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (4.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.10.0.2)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.3.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e13d8d-f780-4e04-a438-c8a080fcbf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import string \n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import text,sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43cc3b-cd8c-4acc-85d6-5a5ccdecc994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab799591-8e1c-426f-ba80-0cb8342465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "client = storage.Client()\n",
    "\n",
    "bucket = \"hackteam-brainpower\"\n",
    "file_name = \"cleaned_data/part-00000-99d78a6d-8a14-4d63-abce-2724aef10f68-c000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca5937c-9b36-4050-bd10-c0797fac33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = client.get_bucket(bucket)\n",
    "blob = bucket.get_blob(file_name)\n",
    "content = blob.download_as_string()\n",
    "\n",
    "df = pd.read_csv(BytesIO(content),on_bad_lines='skip',names=[\"url\", \"type\", \"source\", \"domain\",\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c3174c-24c4-42f3-b2bc-cf9c86b6c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2)\n",
      "(42590, 2)\n",
      "(14031, 2)\n",
      "(0, 2)\n",
      "(90904, 2)\n",
      "(145517, 1)\n",
      "(145517,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11572/3437315668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11572/3437315668.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-Z]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     19\u001b[0m         return [\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         ]\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         ]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "df = df[[\"content\", 'type']]\n",
    "df\n",
    "\n",
    "#Alter the rows \n",
    "df_1 = df[df['type'] == 'conspiracy']\n",
    "print(df_1.shape)\n",
    "df_2 = df[df['type'] == 'fake']\n",
    "print(df_2.shape)\n",
    "df_3 = df[df['type'] == 'unreliable']\n",
    "print(df_3.shape)\n",
    "df_4 = df[df['type'] == 'clickbait']\n",
    "print(df_4.shape)\n",
    "df_5 = df[df['type'] == 'reliable']\n",
    "print(df_5.shape)\n",
    "df_new = pd.concat([df_1, df_2,df_3, df_4, df_5], axis=0)\n",
    "df_new = df_new.reset_index()\n",
    "df_new= df_new.copy()\n",
    "df = df_new\n",
    "df.drop([\"index\"], axis=1)\n",
    "df.head()\n",
    "\n",
    "# df = df.sample(frac =0.025)\n",
    "\n",
    "\n",
    "df = df[[\"content\", 'type']]\n",
    "\n",
    "df.head(3)\n",
    "\n",
    "df = df.dropna()\n",
    "df.shape\n",
    "\n",
    "#def clean_text(text):\n",
    " #   '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    " #   and remove words containing numbers.'''\n",
    " #   text = str(text).lower()\n",
    " #   text = re.sub('\\[.*?\\]', '', text)\n",
    "#    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    " #   text = re.sub('<.*?>+', '', text)\n",
    "  #  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "  #  text = re.sub('\\n', '', text)\n",
    "  #  text = re.sub('\\w*\\d\\w*', '', text)\n",
    "  #  return text\n",
    "\n",
    "\n",
    "\n",
    "# def clean_text_(text):\n",
    "#     '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "#     and remove words containing numbers.'''\n",
    "#     text = str(text).lower()\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\n",
    "#     text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "#     text = re.sub('<.*?>+', '', text)\n",
    "#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "#     text = re.sub('\\n', '', text)\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\n",
    "#     return text\n",
    "# tt = \"Hello wOrld !! WelCome to ( PyTHon's tuTorial ) we wiLL stArt at SHARP 9:00 o'clock\"\n",
    "# new = clean_text(tt)\n",
    "# import nltk\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# word_tokens = word_tokenize(new)\n",
    "# filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "# print(filtered_sentence)\n",
    "# ps = PorterStemmer()\n",
    "# for i in filtered_sentence:\n",
    "#     print(ps.stem(i))\n",
    "\n",
    "df.type.value_counts()\n",
    "\n",
    "df.type = df.type.replace(\"conspiracy\", \"fake\").replace(\"unreliable\", \"fake\").replace(\"clickbait\", \"fake\")\n",
    "df.type = df.type.replace('fake', 0).replace('reliable',1)\n",
    "df.type.value_counts()\n",
    "\n",
    "#df['content'] = df['content'].apply(lambda x:clean_text(x))\n",
    "\n",
    "\n",
    "\n",
    "max_size = 200\n",
    "df['content'] = df['content'].apply(lambda x: ' '.join(x.split(maxsplit=max_size)[:max_size]))\n",
    "df['content']\n",
    "\n",
    "## GETTING INDEPENDENT FEATURES\n",
    "x= df.drop('type',axis=1)\n",
    "\n",
    "# DEPENDENT FEATURE OR TARGET FEATURE\n",
    "y=df['type']\n",
    "\n",
    "# LOOKING AT THE SHAPE OF OUR IINDEPENDENT FEATURES DATASET\n",
    "print(x.shape)\n",
    "\n",
    "# LOOKING AT THE SHAPE OF OUR DEPENDENT FEATURE\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# vocabularry size\n",
    "voc_size=40000\n",
    "\n",
    "# COPYING OUR INDEPENDENT FEATURE DATASET 'x' INTO NEW VARIABLE 'messages'\n",
    "messages= x.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# DOWNLOADING THE STOPWORDS\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# TEXT PREPROCESSING--> STEMMING, REMOVAL OF STOP WORDS, CONVERTING INTO LOWER CASE\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = [] \n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['content'][i])\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus[1]\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "onehot_repr= [one_hot(words,voc_size)for words in corpus]\n",
    "print(onehot_repr[0])\n",
    "\n",
    "# SETTING OUR SENTENCE LENGTH = 100\n",
    "sent_length=100\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)\n",
    "len(embedded_docs)\n",
    "\n",
    "## CREATING OUR LSTM MODEL\n",
    "embedding_vector_features= 40\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length)) # making embedding layer\n",
    "model.add(LSTM(100))  # one LSTM Layer with 100 neurons\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# CONVERTING X AND Y INTO ARRAYS\n",
    "x_final= np.array(embedded_docs)\n",
    "y_final= np.array(y)\n",
    "x_final.shape, y_final.shape\n",
    "\n",
    "\n",
    "# SPLITTING THE DATA INTO TRAINING AND TEST SETS\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_final,y_final,test_size=0.33,random_state=0)\n",
    "\n",
    "print(x_train.shape, y_train.shape , x_test.shape, y_test.shape)\n",
    "\n",
    "# FITTING NTO THE MODEL\n",
    "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,batch_size=64, verbose = True)\n",
    "\n",
    "# OUR PREDICTION VARIABLE\n",
    "y_pred= model.predict_proba(x_test)\n",
    "y_pred = [int(i > .5) for i in y_pred]\n",
    "\n",
    "# PRINTING CONFUSION MATRIX\n",
    "# IMPORTING LIBRARIES TO SEE THE ACCURACY\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm= confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "\n",
    "# ACCURACY SCORE\n",
    "ac= classification_report(y_test,y_pred)\n",
    "print(ac)\n",
    "\n",
    "#from keras.models import load_model\n",
    "\n",
    "m#odel.save('./kaggle/working/my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec9a05-4a13-4085-b2f8-7a4031efe042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
